## 爬蟲框架Scrapy簡介

當你寫了很多個爬蟲程式之後，你會發現每次寫爬蟲程式時，都需要將頁面獲取、頁面解析、爬蟲排程、異常處理、反爬應對這些程式碼從頭至尾實現一遍，這裡面有很多工作其實都是簡單乏味的重複勞動。那麼，有沒有什麼辦法可以提升我們編寫爬蟲程式碼的效率呢？答案是肯定的，那就是利用爬蟲框架，而在所有的爬蟲框架中，Scrapy 應該是最流行、最強大的框架。

### Scrapy 概述

Scrapy 是基於 Python 的一個非常流行的網路爬蟲框架，可以用來抓取 Web 站點並從頁面中提取結構化的資料。下圖展示了 Scrapy 的基本架構，其中包含了主要元件和系統的資料處理流程（圖中帶數字的紅色箭頭）。

![](https://gitee.com/jackfrued/mypic/raw/master/20210824003638.png)

#### Scrapy的元件

我們先來說說 Scrapy 中的元件。

1. Scrapy 引擎（Engine）：用來控制整個系統的資料處理流程。
2. 排程器（Scheduler）：排程器從引擎接受請求並排序列入佇列，並在引擎發出請求後返還給它們。
3. 下載器（Downloader）：下載器的主要職責是抓取網頁並將網頁內容返還給蜘蛛（Spiders）。
4. 蜘蛛程式（Spiders）：蜘蛛是使用者自定義的用來解析網頁並抓取特定URL的類，每個蜘蛛都能處理一個域名或一組域名，簡單的說就是用來定義特定網站的抓取和解析規則的模組。
5. 資料管道（Item Pipeline）：管道的主要責任是負責處理有蜘蛛從網頁中抽取的資料條目，它的主要任務是清理、驗證和儲存資料。當頁面被蜘蛛解析後，將被髮送到資料管道，並經過幾個特定的次序處理資料。每個資料管道元件都是一個 Python 類，它們獲取了資料條目並執行對資料條目進行處理的方法，同時還需要確定是否需要在資料管道中繼續執行下一步或是直接丟棄掉不處理。資料管道通常執行的任務有：清理 HTML 資料、驗證解析到的資料（檢查條目是否包含必要的欄位）、檢查是不是重複資料（如果重複就丟棄）、將解析到的資料儲存到資料庫（關係型資料庫或 NoSQL 資料庫）中。
6. 中介軟體（Middlewares）：中介軟體是介於引擎和其他元件之間的一個鉤子框架，主要是為了提供自定義的程式碼來拓展 Scrapy 的功能，包括下載器中介軟體和蜘蛛中介軟體。

#### 資料處理流程

Scrapy 的整個資料處理流程由引擎進行控制，通常的運轉流程包括以下的步驟：

1. 引擎詢問蜘蛛需要處理哪個網站，並讓蜘蛛將第一個需要處理的 URL 交給它。

2. 引擎讓排程器將需要處理的 URL 放在佇列中。

3. 引擎從排程那獲取接下來進行爬取的頁面。

4. 排程將下一個爬取的 URL 返回給引擎，引擎將它透過下載中介軟體傳送到下載器。

5. 當網頁被下載器下載完成以後，響應內容透過下載中介軟體被髮送到引擎；如果下載失敗了，引擎會通知排程器記錄這個 URL，待會再重新下載。

6. 引擎收到下載器的響應並將它透過蜘蛛中介軟體傳送到蜘蛛進行處理。

7. 蜘蛛處理響應並返回爬取到的資料條目，此外還要將需要跟進的新的 URL 傳送給引擎。

8. 引擎將抓取到的資料條目送入資料管道，把新的 URL 傳送給排程器放入佇列中。

上述操作中的第2步到第8步會一直重複直到排程器中沒有需要請求的 URL，爬蟲就停止工作。

### 安裝和使用Scrapy

可以使用 Python 的包管理工具`pip`來安裝 Scrapy。

```Shell
pip install scrapy
```

在命令列中使用`scrapy`命令建立名為`demo`的專案。

```Bash
scrapy startproject demo
```

專案的目錄結構如下圖所示。

```Shell
demo
|____ demo
|________ spiders
|____________ __init__.py
|________ __init__.py
|________ items.py
|________ middlewares.py
|________ pipelines.py
|________ settings.py
|____ scrapy.cfg
```

切換到`demo` 目錄，用下面的命令建立名為`douban`的蜘蛛程式。

```Bash
scrapy genspider douban movie.douban.com
```

#### 一個簡單的例子

接下來，我們實現一個爬取豆瓣電影 Top250 電影標題、評分和金句的爬蟲。

1. 在`items.py`的`Item`類中定義欄位，這些欄位用來儲存資料，方便後續的操作。

   ```Python
   import scrapy
   
   
   class DoubanItem(scrapy.Item):
       title = scrapy.Field()
       score = scrapy.Field()
       motto = scrapy.Field()
   ```
   
2. 修改`spiders`資料夾中名為`douban.py` 的檔案，它是蜘蛛程式的核心，需要我們新增解析頁面的程式碼。在這裡，我們可以透過對`Response`物件的解析，獲取電影的資訊，程式碼如下所示。

   ```Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
       start_urls = ['https://movie.douban.com/top250?start=0&filter=']
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   ```
   透過上面的程式碼不難看出，我們可以使用 CSS 選擇器進行頁面解析。當然，如果你願意也可以使用 XPath 或正則表示式進行頁面解析，對應的方法分別是`xpath`和`re`。

   如果還要生成後續爬取的請求，我們可以用`yield`產出`Request`物件。`Request`物件有兩個非常重要的屬性，一個是`url`，它代表了要請求的地址；一個是`callback`，它代表了獲得響應之後要執行的回撥函式。我們可以將上面的程式碼稍作修改。

   ```Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
       start_urls = ['https://movie.douban.com/top250?start=0&filter=']
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   
           hrefs = sel.css('#content > div > div.article > div.paginator > a::attr("href")')
           for href in hrefs:
               full_url = response.urljoin(href.extract())
               yield Request(url=full_url)
   ```

   到這裡，我們已經可以透過下面的命令讓爬蟲運轉起來。

   ```Shell
   scrapy crawl movie
   ```

   可以在控制檯看到爬取到的資料，如果想將這些資料儲存到檔案中，可以透過`-o`引數來指定檔名，Scrapy 支援我們將爬取到的資料匯出成 JSON、CSV、XML 等格式。

   ```Shell
   scrapy crawl moive -o result.json
   ```

   不知大家是否注意到，透過執行爬蟲獲得的 JSON 檔案中有`275`條資料，那是因為首頁被重複爬取了。要解決這個問題，可以對上面的程式碼稍作調整，不在`parse`方法中解析獲取新頁面的 URL，而是透過`start_requests`方法提前準備好待爬取頁面的 URL，調整後的程式碼如下所示。

   ```Python
   import scrapy
   from scrapy import Selector, Request
   from scrapy.http import HtmlResponse
   
   from demo.items import MovieItem
   
   
   class DoubanSpider(scrapy.Spider):
       name = 'douban'
       allowed_domains = ['movie.douban.com']
   
       def start_requests(self):
           for page in range(10):
               yield Request(url=f'https://movie.douban.com/top250?start={page * 25}')
   
       def parse(self, response: HtmlResponse):
           sel = Selector(response)
           movie_items = sel.css('#content > div > div.article > ol > li')
           for movie_sel in movie_items:
               item = MovieItem()
               item['title'] = movie_sel.css('.title::text').extract_first()
               item['score'] = movie_sel.css('.rating_num::text').extract_first()
               item['motto'] = movie_sel.css('.inq::text').extract_first()
               yield item
   ```

3. 如果希望完成爬蟲資料的持久化，可以在資料管道中處理蜘蛛程式產生的`Item`物件。例如，我們可以透過前面講到的`openpyxl`操作 Excel 檔案，將資料寫入 Excel 檔案中，程式碼如下所示。

   ```Python
   import openpyxl
   
   from demo.items import MovieItem
   
   
   class MovieItemPipeline:
   
       def __init__(self):
           self.wb = openpyxl.Workbook()
           self.sheet = self.wb.active
           self.sheet.title = 'Top250'
           self.sheet.append(('名稱', '評分', '名言'))
   
       def process_item(self, item: MovieItem, spider):
           self.sheet.append((item['title'], item['score'], item['motto']))
           return item
   
       def close_spider(self, spider):
           self.wb.save('豆瓣電影資料.xlsx')
   ```

   上面的`process_item`和`close_spider`都是回撥方法（鉤子函式）， 簡單的說就是 Scrapy 框架會自動去呼叫的方法。當蜘蛛程式產生一個`Item`物件交給引擎時，引擎會將該`Item`物件交給資料管道，這時我們配置好的資料管道的`parse_item`方法就會被執行，所以我們可以在該方法中獲取資料並完成資料的持久化操作。另一個方法`close_spider`是在爬蟲結束執行前會自動執行的方法，在上面的程式碼中，我們在這個地方進行了儲存 Excel 檔案的操作，相信這段程式碼大家是很容易讀懂的。

   總而言之，資料管道可以幫助我們完成以下操作：

   - 清理 HTML 資料，驗證爬取的資料。
   - 丟棄重複的不必要的內容。
   - 將爬取的結果進行持久化操作。

4. 修改`settings.py`檔案對專案進行配置，主要需要修改以下幾個配置。

   ```Python
   # 使用者瀏覽器
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
   
   # 併發請求數量 
   CONCURRENT_REQUESTS = 4
   
   # 下載延遲
   DOWNLOAD_DELAY = 3
   # 隨機化下載延遲
   RANDOMIZE_DOWNLOAD_DELAY = True
   
   # 是否遵守爬蟲協議
   ROBOTSTXT_OBEY = True
   
   # 配置資料管道
   ITEM_PIPELINES = {
      'demo.pipelines.MovieItemPipeline': 300,
   }
   ```

   > **說明**：上面配置檔案中的`ITEM_PIPELINES`選項是一個字典，可以配置多個處理資料的管道，後面的數字代表了執行的優先順序，數字小的先執行。

