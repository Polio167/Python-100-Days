## 機率統計基礎

機率論源於賭博遊戲。義大利文藝復興時代，百科全書式的學者卡爾達諾（死後）發表的《論賭博遊戲》被認為是第一部論述機率論的著作。到了17世紀的法國，宮廷貴族裡盛行著擲骰子游戲，遊戲規則是玩家連續擲4次骰子，如果其中沒有6點出現，玩家贏，如果出現一次6點，則莊家（相當於現在的賭場）贏；後來為了使遊戲更刺激，遊戲規則發生了些許變化，玩家用2個骰子連續擲24次，不同時出現2個6點，玩家贏，否則莊家贏。在這樣的時代背景下，法國數學家帕斯卡和費馬創立了機率論，後來雅各布·伯努利發現，機率論遠遠不止用於賭博，他將他的思考和研究記錄下來，寫成了《猜度數》一書，提出了大數定理（**在一個隨機事件中，隨著試驗次數的增加，事件發生的頻率越趨近於一個穩定值**），這個定理在當時的保險公司得到了充分利用。

> **思考**：回到剛才的骰子游戲，按照舊玩法，莊家獲勝的機率是多少？換成新玩法，莊家獲勝的機率與之前的玩法相同嗎？

以機率論為基礎的統計學顯然比機率論出現的時間更晚，而且一直以來都是一種尷尬的存在，處於各種鄙視鏈的底端。從數學的角度看，統計學中的數學原理過於膚淺；從應用科學的角度看，統計學太數學，跟應用沾不上邊。盧瑟福（原子核物理學之父）曾經說過，“如果你的實驗需要統計學，那麼你應該再做一個更好的實驗”；波普爾（20世紀最偉大的哲學家之一）也曾經對歸納邏輯進行過強烈的抨擊。曾經，由於歐幾里得、笛卡爾帶給人們的完美體系實在太過迷人，導致很多人都忽視了統計思維這一重要的科學思維方式。但是最近十年時間，隨著大資料和人工智慧時代的來臨，統計學又以驚人的速度流行起來，因為大資料時代已經充分證明了經驗主義、歸納推理的強大之處；而人工智慧實際上也是大資料加上深度學習的歸納方法所取得的成功。有了統計學，我們能夠處有效的處理海量的資料，也能夠正確理解資料分析的結果。

按照統計方法的不同，我們可以將統計學分類為描述統計學、推斷統計學、貝葉斯統計學等。描述統計學是用來描繪或總結觀察量的基本情況的統計方法，通常會將整理後的資料做成表格或圖表，具體包括資料的集中趨勢分析、離散趨勢分析和相關分析。推斷統計學是研究如何根據樣本資料推斷總體資料特徵的方法，在無法獲得全量資料的情況下，推斷統計就是最為行之有效的方法。貝葉斯統計學的基礎是貝葉斯定理，貝葉斯定理將經驗和直覺與機率相關聯，和人類大腦的判斷原理十分類似，簡單的說就是在獲取到新的資料之後，先前憑藉經驗和直覺獲得的機率是可以改變的。

### 資料和資料的分類

在統計學中，透過試驗、觀察、調查等獲得的材料被稱為資料，資料大致可以分為定性資料和定量資料，其中定性資料又可以分為定類尺度和定序尺度，定量資料又可以分為定距尺度和定比尺度，如下表所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20220320232245.png" style="zoom:50%">

####定性資料的處理

1. 定類尺度（名義尺度）：定類尺度通常會處理成虛擬變數（啞變數），多個不同的型別最終會變成一個虛擬變數矩陣。
2. 定序尺度：定序尺度可以處理成一個序號，並透過該序號表示等級的高低。

#### 定量資料的處理

1. 線性歸一化
    $$ x_i' = \frac {x_i - min(X)} {max(X) - min(X)} $$

2. 零均值歸一化
    $$ x_i' = \frac {x_i - \mu} {\sigma} $$

### 資料的集中趨勢

我們經常會使用以下幾個指標來描述一組資料的集中趨勢：

1. 均值 - 均值代表某個資料集的整體水平，我們經常提到的客單價、平均訪問時長、平均配送時長等指標都是均值。均值是對資料進行概括的一個強有力的方法，將大量的資料濃縮成了一個數據。均值的缺點是容易受極值的影響，可以使用加權平均值或去尾平均值來消除極值的影響；對於正數可以用幾何平均值來替代算術平均值。
    - 算術平均值：$$\bar{x} = \frac{\sum_{i=1}^{n} {x_{i}}} {n} = \frac{x_{1}+x_{2}+\cdots +x_{n}}{n}$$，例如計算最近30天日均DAU、日均新增訪客等，都可以使用算術平均值。
    - 幾何平均值：$$\left(\prod_{i=1}^{n}x_{i}\right)^{\frac{1}{n}}={\sqrt[{n}]{x_{1}x_{2} \cdots x_{n}}}$$，例如計算不同渠道的平均轉化率、不同客群的平均留存率、不同品類的平均付費率等，就可以使用幾何平均值。
2. 中位數 - 將資料按照升序或降序排列後位於中間的數，它描述了資料的中等水平。中位數的計算分兩種情況：
    - 當資料體量$n$為奇數時，中位數是位於$\frac{n + 1}{2}$位置的元素。
    - 當資料體量$n$為偶數時，中位數是位於$\frac{n}{2}$和${\frac{n}{2}+1}$兩個位置元素的均值。
3. 眾數 - 資料集合中出現頻次最多的資料，它代表了資料的一般水平。一般在資料量比較大時，眾數才有意義，而且資料越集中，眾數的代表性就越好。眾數不受極值的影響，但是無法保證唯一性和存在性。

例子：有A和B兩組資料。

```
A組：5, 6, 6, 6, 6, 8, 10
B組：3, 5, 5, 6, 6, 9, 12
```

A組的均值：6.74，中位數：6，眾數：6。

B組的均值：6.57，中位數：6，眾數：5, 6。

> **說明**：在Excel中，可以使用`AVERAGE`、`GEOMEAN`、`MEDIAN`、`MODE.SNGL`、`MODE.MULT`函式分別計算均值、中位數和眾數。求中位數也可以使用`QUARTILE.EXC`或`QUARTILE.INC`函式，將第二個引數設定為2即可。

對A組的資料進行一些調整。

```
A組：5, 6, 6, 6, 6, 8, 10, 500
B組：3, 5, 5, 6, 6, 9, 12
```

A組的均值會大幅度提升，但中位數和眾數卻沒有變化。

|        | 優點                             | 缺點                                 |
| ------ | -------------------------------- | ------------------------------------ |
| 均值   | 充分利用了所有資料，適應性強     | 容易收到極端值（異常值）的影響       |
| 中位數 | 能夠避免被極端值（異常值）的影響 | 不敏感                               |
| 眾數   | 能夠很好的反映資料的集中趨勢     | 有可能不存在（資料沒有明顯集中趨勢） |

### 資料的離散趨勢

如果說資料的集中趨勢，說明了資料最主要的特徵是什麼；那麼資料的離散趨勢，則體現了這個特徵的穩定性。簡單的說就是資料越集中，均值的代表性就越強；資料波動越大，均值的代表性就越弱。

1. 極值：就是最大值（maximum）、最小值（minimum），代表著資料集的上限和下限。

    > **說明**：Excel 中，計算極值的函式分別是`MAX`和`MIN`。

2. 極差：又稱“全距”，是一組資料中的最大觀測值和最小觀測值之差，記作$R$。一般情況下，極差越大，離散程度越大，資料受極值的影響越嚴重。

3. 四分位距離：$\small{IQR = Q_3 - Q_1}$。

    > **提示**：箱線圖。

4. 方差：將每個值與均值的偏差進行平方，然後除以總資料量得到的值。簡單來說就是表示資料與期望值的偏離程度。方差越大，就意味著資料越不穩定、波動越劇烈，因此代表著資料整體比較分散，呈現出離散的趨勢；而方差越小，意味著資料越穩定、波動越平滑，因此代表著資料整體比較集中。簡單的總結一下，
    - 總體方差：$$ \sigma^2 = \frac {\sum_{i=1}^{N} {(X_i - \mu)^2}} {N} $$。
    - 樣本方差：$$ S^2 = \frac {\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1} $$。

    > **說明**：Excel 中，計算總體方差和樣本方差的函式分別是`VAR.P`和`VAR.S`。

5. 標準差：將方差進行平方根運算後的結果，與方差一樣都是表示資料與期望值的偏離程度。
    - 總體標準差：$$ \sigma = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \mu)^2}} {N}} $$
    - 樣本標準差：$$ S = \sqrt{\frac{\sum_{i=1}^{N} {(X_i - \bar{X})^2}} {N-1}} $$

    > **說明**：Excel 中，計算標準差的函式分別是`STDEV.P`和`STDEV.S`。

### 資料的頻數分析

用一定的方式將資料分組，然後統計每個分組中樣本的數量，再輔以圖表（如直方圖）就可以更直觀的展示資料分佈趨勢的一種方法。

頻數分析的業務意義：

1. 大問題變小問題，迅速聚焦到需要關注的群體。
2. 找到合理的分類機制，有利於長期的資料分析（維度拆解）。

例如：一個班有50個學生，考試成績如下所示：

```
87,  80,  79,  78,  55,  80,  81,  60,  78,  82,  67,  74,  67, 74,  66,  91,  100,  70,  82,  71,  77,  94,  75,  83,  85,  84, 47,  75,  84,  96,  53,  86,  86,  89,  71,  76,  75,  80,  70,  83,  77,  91,  90,  82,  74,  74,  78,  53,  88,  72
```

在獲得資料後，我們先解讀資料的集中趨勢和離散趨勢。

均值：`77.4`，中位數：`78.0`，眾數：`74`。

最高分：`100`，最低分：`47`，極差：`53`，方差：`120.16`。

但是，僅僅依靠上面的指標是很難對一個數據集做出全面的解讀，我們可以把學生按照考試成績進行分組，如下所示。

| 分數段   | 學生人數 |
| -------- | -------- |
| <60      | 4        |
| [60, 65) | 1        |
| [65, 70) | 3        |
| [70, 75) | 9        |
| [75, 80) | 10       |
| [80, 85) | 11       |
| [85, 90) | 6        |
| [90, 95) | 4        |
| >=95     | 2        |

我們可以利用直方圖來檢視資料分佈的形態，對資料分佈形態的測度主要以正態分佈為標準進行衡量，正態分佈在座標軸上的形狀是一個鈴鐺型（鍾型），正態曲線以均值為中心左右對稱，如下圖所示，而上面的學生考試成績資料就呈現出正態分佈的輪廓。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155507.png" width="80%">

我們可以資料分佈的直方圖擬合出一條曲線與正態曲線進行比較，主要比較曲線的尖峭程度和對稱性，通常稱之為峰度和偏態。資料分佈的不對稱性稱為偏態，偏態又分為正偏（右偏）或負偏（左偏）兩種。在正態分佈的情況下，中位數和均值應該都在對稱軸的位置，如果中位數在左邊，均值在右邊，那麼資料的極端值也在右邊，資料分佈曲線向右延伸，就是我們說的右偏；如果均值在左邊，中位數在右邊，那麼資料的極端值在左邊，資料分佈曲線向左延伸，就是我們說的左偏。測定偏態的指標是偏態係數，Excel 中計算偏度係數使用的公式如下所示：
$$
SK = \frac{n}{(n - 1)(n - 2)} \sum(\frac{x_i - \bar{x}}{s})^3
$$
$\small{SK > 0}$時，分佈呈現正偏，SK值越大，正偏程度越高。

$\small{SK < 0}$時，分佈呈現負偏，SK值越小，負偏程度越高。

峰度是指資料分佈的尖峭程度，一般可以表現為尖頂峰度、平頂峰度和標準峰度（正態分佈的峰度）。測定峰度的指標是峰度係數，Excel 中計算峰度係數使用的公式如下所示：
$$
K = \frac{n(n + 1)}{(n - 1)(n - 2)(n - 3)}\sum(\frac{x_i - \bar{x}}{s})^4-\frac{3(n - 1)^2}{(n - 2)(n - 3)}
$$
峰度係數$\small{K < 0}$時，分佈與正態分佈相比更為扁平、寬肩、瘦尾；峰度係數$\small{K > 0}$時，分佈與正態分佈相比更為尖峰、瘦肩、肥尾。

### 資料的機率分佈

#### 基本概念

1. 隨機現象：在一定條件下可能發生也可能不發生，結果具有偶然性的現象。

2. 樣本空間（*sample space*）：隨機現象一切可能的結果組成的集合。

  - 拋一枚硬幣的樣本空間：$\Omega = \{ \omega_1, \omega_2 \}$。
  - 拋兩枚硬幣的樣本空間：$\Omega = \{ \omega_1, \omega_2, \omega_3, \omega_4 \}$，其中$\omega_1 = (H, H)$，$\omega_2 = (H, T)$，$\omega_3 = (T, H)$，$\omega_4 = (T, T)$。
  - 離散型的樣本空間的元素是可列的，連續型的樣本空間的元素是（無限）不可列的。

3. 隨機試驗（*trials*）：在相同條件下對某種隨機現象進行觀測的試驗。隨機試驗滿足三個特點：

    - 可以在相同條件下重複的進行。

    - 每次試驗的結果不止一個，事先可以明確指出全部可能的結果。

    - 重複試驗的結果以隨機的方式出現（事先不確定會出現哪個結果）。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075000.png" style="zoom:75%">

4. 隨機變數（*random variable*）：如果$X$指定給機率空間$S$中每一個事件$e$有一個實數$X(e)$，同時針對每一個實數$r$都有一個事件集合$A_r$與其相對應，其中$A_r=\{e: X(e) \le r\}$，那麼$X$被稱作隨機變數。從這個定義看出，$X$的本質是一個實值函式，以給定事件為自變數的實值函式，因為函式在給定自變數時會產生因變數，所以將$X$稱為隨機變數。簡單的說，隨機變數的值需要透過試驗來確認。

    - 離散型隨機變數：資料可以一一列出。
    - 連續型隨機變數：資料不可以一一列出。

    > **說明**：如果離散型隨機變數的取值非常龐大時，可以近似看做連續型隨機變數。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075148.png" style="zoom:50%;">

    <img src="https://github.com/jackfrued/mypic/raw/master/20220322075331.png" style="zoom:50%;">

5. 機率（*probability*）：用一個0~1之間的數字表示隨機現象發生的可能性，也就是說機率是隨機事件出現可能性的度量。

6. 機率質量函式/機率密度函式：機率質量函式是描述離散型隨機變數為特定取值的機率的函式，通常縮寫為**PMF**。機率密度函式是描述連續型隨機變數在某個確定的取值點可能性的函式，通常縮寫為**PDF**。二者的區別在於，機率密度函式本身不是機率，只有對機率密度函式在某區間內進行積分後才是機率。

7. 隨機變數的數字特徵：

    - （數學）期望：隨機變數按照機率的加權平均，它表示了機率分佈的中心位置，反映隨機變數平均取值的大小。

        對於離散型隨機變數$ X $，若$ \sum_{i=1}^{\infty} x_ip_i $收斂，那麼它就是隨機變數$ X $的期望，記為$ E(X) $，即$ E(X) = \sum_{i=1}^{\infty} x_ip_i $，否則隨機變數$ X $的期望不存在。

        對於連續型隨機變數$ X $，其機率密度函式為$ f(x) $，若$ \int_{-\infty}^{\infty}xf(x)dx $收斂，則稱$ E(x) =  \int_{-\infty}^{\infty}xf(x)dx $為隨機變數$ X $的數學期望，否則隨機變數$ X $的期望不存在。

    - 方差：方差用來表示隨機變數機率分佈的離散程度，對於隨機變數$ X $，若$ E((X - E(X))^2) $存在，則稱$ E((X - E(X))^2) $為$ X $的方差，記為$ Var(X) $。很顯然，離散型隨機變數$ X $的方差為$ Var(X) = \sum_{i=1}^{\infty} [x_i - E(X)]^2p_i$，連續型隨機變數$ X $的方差為$ Var(X) = \int_{-\infty}^{\infty} [x - E(X)]^2f(x)dx $。

8. 期望與方差的性質：

    - 對於任意兩個隨機變數$ X_1 $和$ X_2 $，則有$ E(X_1 + X_2) = E(X_1) + E(X_2) $。
    - 若$ X $是隨機變數，$ a $和$ b $是任意常量，則有$ E(aX + b) = aE(X) + b $和$ Var(aX + b) = a^2Var(X)$。
    - 若隨機變數$ X_1 $和$ X_2 $獨立，則有$ Var(X_1 + X_2) = Var(X_1) + Var(X_2) $。

9. 其他零碎小概念：

    - 互斥（*mutually exclusive*）：事件不能同時發生。
    - 獨立（*independant*）：一個試驗的結果不會對另一個試驗的結果產生影響。
    - 排列（permutation）：$ P_k^n = \frac{n!}{(n - k)!} $，國內教科書一般記為$ P_n^k $或$ A_n^k $。
    - 組合（*combination*）：$ C_k^n = \frac{n!}{k!(n-k)!} $，國內教科書一般記為$ C_n^k $。

#### 離散型分佈

1. 伯努利分佈（*Bernoulli distribution*）：又名**兩點分佈**或者**0-1分佈**，是一個離散型機率分佈。若伯努利試驗成功，則隨機變數取值為1。若伯努利試驗失敗，則隨機變數取值為0。記其成功機率為$ p (0 \le p \le 1) $，失敗機率為$ q=1-p $，則機率質量函式為：

    $$ f(x)=p^{x}(1-p)^{1-x}=\left\{{\begin{matrix}p&{\mbox{if }}x=1,\\q\ &{\mbox{if }}x=0.\\\end{matrix}}\right. $$

2. 二項分佈（*Binomial distribution*）：$n$個獨立的是/非試驗中成功次數的離散機率分佈，其中每次試驗的成功機率為$p$。一般地，如果隨機變數$X$服從引數為$ n $和$ p $的二項分佈，記為$ X\sim B(n,p) $。$ n $次試驗中正好得到$ k $次成功的機率由機率質量函式給出，
    $$ P(X=k) = C_k^np^k(1-p)^{n-k} $$

    > **提示**：Excel 中，可以透過`BINOM.DIST.RANGE`函式計算二項分佈的機率。

3. 泊松分佈（*Poisson distribution*）：適合於描述單位時間內隨機事件發生的次數的機率分佈。如某一服務設施在一定時間內受到的服務請求的次數、汽車站臺的候客人數、機器出現的故障數、自然災害發生的次數、DNA序列的變異數、放射性原子核的衰變數等等。泊松分佈的機率質量函式為：$P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}$，泊松分佈的引數$\lambda$是單位時間（或單位面積）內隨機事件的平均發生率。

    > **說明**：泊松分佈是在沒有計算機的年代，由於二項分佈的運算量太大運算比較困難，為了減少運算量，數學家為二項分佈提供的一種近似。當二項分佈的$n$很大，$p$很小的時候，我們可以讓$\lambda = np$，然後用泊松分佈的機率質量函式計算機率來近似二項分佈的機率。

#### 分佈函式

對於連續型隨機變數，我們不可能去羅列每一個值出現的機率，因此要引入分佈函式的概念。
$$
F(x) = P\{X \le x\}
$$
如果將$ X $看成是數軸上的隨機座標，上面的分佈函式表示了$ x $落在區間$ (-\infty, x) $中的機率。分佈函式有以下性質：

1. $ F(x) $是一個單調不減的函式；
2. $ 0 \le F(x) \le 1$，且$ F(-\infty) = \lim_{x \to -\infty} F(x) = 0 $， $F(\infty) = \lim_{x \to \infty} F(x) = 1$；
3. $ F(x) $是右連續的。

機率密度函式就是給分佈函式求導的結果，簡單的說就是：
$$
F(x) = \int_{- \infty}^{x} f(t)dt
$$

#### 連續型分佈

1. 均勻分佈（*Uniform distribution*）：如果連續型隨機變數$X$具有機率密度函式$f(x)=\begin{cases}{\frac{1}{b-a}} \quad &{a \leq x \leq b} \\ {0} \quad &{\mbox{other}}\end{cases}$，則稱$X$服從$[a,b]$上的均勻分佈，記作$X\sim U[a,b]$。

2. 指數分佈（*Exponential distribution*）：如果連續型隨機變數$X$具有機率密度函式$f(x)=\begin{cases} \lambda e^{- \lambda x} \quad &{x \ge 0} \\ {0} \quad &{x \lt 0} \end{cases}$，則稱$X$服從引數為$\lambda$的指數分佈，記為$X \sim Exp(\lambda)$。指數分佈可以用來表示獨立隨機事件發生的時間間隔，比如旅客進入機場的時間間隔、客服中心接入電話的時間間隔、知乎上出現新問題的時間間隔等等。指數分佈的一個重要特徵是無記憶性（無後效性），這表示如果一個隨機變數呈指數分佈，它的條件機率遵循：$P(T \gt s+t\ |\ T \gt t)=P(T \gt s), \forall s,t \ge 0$。

3. 正態分佈（*Normal distribution*）：又名**高斯分佈**（*Gaussian distribution*），是一個非常常見的連續機率分佈，經常用自然科學和社會科學中來代表一個不明的隨機變數。若隨機變數$X$服從一個位置引數為$\mu$、尺度引數為$\sigma$的正態分佈，記為$X \sim N(\mu,\sigma^2)$，其機率密度函式為：$\displaystyle f(x)={\frac {1}{{\sqrt {2 \pi } \sigma}}}e^{-{\frac {\left(x-\mu \right)^{2}}{2\sigma ^{2}}}}$。

    根據“棣莫弗-拉普拉斯積分定理”，假設$ \mu_{n} (n=1, 2, \cdots) $表示$ n $重伯努利試驗中成功的次數，已知每次試驗成功的機率為$p$，那麼：
    $$ \lim_{n \to \infty} P\lbrace \frac{\mu_n - np} {\sqrt{np(1-p)}} \le x \rbrace = \frac {1} {\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac {\mu^2} {2}} dx $$，該定理表明正態分佈是二項分佈的極限分佈。
    

提到正態分佈，就必須說一下“3$\sigma$法則”，該法則也稱為“68-95-99.7”法則，如下圖所示。

<img src="https://github.com/jackfrued/mypic/raw/master/20210716155542.png" style="zoom:65%">

正態分佈有一個非常重要的性質，**大量統計獨立的隨機變數的平均值的分佈趨於正態分佈**，即$ \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) $這就是**中心極限定理**。中心極限定理的重要意義在於，我們可以用正態分佈作為其他機率分佈的近似。

一個例子：假設某校入學新生的智力測驗平均分數與標準差分別為 100 與 12。那麼隨機抽取 50 個學生，他們智力測驗平均分數大於 105 的機率是多少？小於 90 的機率是多少？

本例沒有正態分佈的假設，還好中心極限定理提供一個可行解，那就是當隨機樣本數量超過30，樣本平均數近似於一個正態變數，我們可以構造標準正態變數$ Z = \frac {\bar{X} - \mu} {\sigma / \sqrt{n}} $。

平均分數大於 105 的機率為：$ P(Z \gt \frac{105 - 100}{12 / \sqrt{50}}) = P(Z \gt 5/1.7) = P(Z \gt 2.94) = 0.0016$。

平均分數小於 90 的機率為：$ P(Z \lt \frac{90-100}{12/\sqrt{50}}) = P(Z < -5.88) = 0.0000 $。
    

> **說明**：上面標準正態分佈的機率值可以查表得到，在 Excel 中可以使用`NORM.DIST`函式獲得。例如在上面的例子中，我們可以透過`NORM.DIST(2.94, 0, 1, TRUE)`獲得$P(z\le2.94)$的機率為`0.998359`。

#### 基於正態分佈的三大分佈

1. 卡方分佈（*Chi-square distribution*）：若$k$個隨機變數$Z_1,Z_2,...,Z_k$是相互獨立且服從標準正態分佈$N(0, 1)$的隨機變數，則隨機變數$X = \sum_{i=1}^{k}Z_i^2$被稱為服從自由度為$k$的卡方分佈，記為$X \sim \chi^2(k)$。卡方分佈的機率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323201608.png" style="zoom:50%;">

2. $t$分佈：設$X \sim N(0, 1)$， $Y \sim {\chi}^2(n)$，且$X$與$Y$相互獨立，則隨機變數$T = \frac {X} {\sqrt{Y/n}}$稱為自由度為$n$的$t$分佈，記作$T \sim t(n)$。$t$分佈的機率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220323203530.png" style="zoom:50%">

3. $F$分佈：設$X \sim \chi^2(n_1)$，$Y \sim \chi^2(n_2)$，且$X$與$Y$相互獨立，則隨機變數$F = \frac{X / n_1}{Y / n_2}$稱為自由度為$(n_1, n_2)$的$F$分佈，記作$F \sim F(n_1, n_2)$，它的機率密度曲線如下所示。

    <img src="https://github.com/jackfrued/mypic/raw/master/20220619164716.png" style="zoom: 50%;">

這三個分佈有什麼用呢？

1. $ \chi^2 $分佈：常用於獨立性檢驗、擬合優度檢驗。
2. $ F $分佈：常用於比例的估計和檢驗，方差分析和迴歸分析中也會用到$ F $分佈。
3. $ t $分佈：在資訊不足的情況下，要對總體均值進行估計和檢驗，就會使用到$ t $分佈。

### 其他內容

#### 貝葉斯定理

**聯合機率**是指事件A和事件B共同發生的機率，通常記為$\small{P(A \cap B)}$。

**條件機率**是指事件A在事件B發生的條件下發生的機率，通常記為$\small{P(A|B)}$。設A與B為樣本空間$\Omega$中的兩個事件，其中$\small{P(B) \gt 0}$。那麼在事件B發生的條件下，事件A發生的條件機率為：${P(A|B)=\frac{P(A \cap B)}{P(B)}}$，當$ P(B)=0 $時，規定$ P(A|B) = 0 $。

> **思考**：
>
> 1. 某家庭有兩個孩子，問兩個孩子都是女孩的機率是多少？
> 2. 某家庭有兩個孩子，已知其中一個是女孩，問兩個孩子都是女孩的機率是多少？
> 3. 某家庭有兩個孩子，已知老大是女孩，問兩個孩子都是女孩的機率是多少？

事件A在事件B已發生的條件下發生的機率，與事件B在事件A已發生的條件下發生的機率是不一樣的。然而，這兩者是有確定的關係的，**貝葉斯定理**就是對這種關係的陳述，如下所示：
$$
P(A|B)=\frac{P(B|A)}{P(B)}P(A)
$$

- $P(A|B)$是已知$B$發生後，$A$的條件機率，也稱為$A$的後驗機率。
- $P(A)$是$A$的先驗機率也稱作邊緣機率，是不考慮$B$時$A$發生的機率。
- $P(B|A)$是已知$A$發生後，$B$的條件機率，稱為$B$的似然性。
- $P(B)$是$B$的先驗機率。

#### 大數定理

樣本數量越多，則其算術平均值就有越高的機率接近期望值。

1. 弱大數定律（辛欽定理）：樣本均值依機率收斂於期望值，即對於任意正數$\epsilon$，有：$\lim_{n \to \infty}P(|\bar{X_n}-\mu|>\epsilon)=0$。
2. 強大數定律：樣本均值以機率1收斂於期望值，即：$P(\lim_{n \to \infty}\bar{X_n}=\mu)=1$。

#### 假設檢驗

假設檢驗就是透過抽取樣本資料，並且透過**小機率反證法**去驗證整體情況的方法。假設檢驗的核心思想是小機率反證法（首先假設想推翻的命題是成立的，然後試圖找出矛盾，找出不合理的地方來證明命題為假命題），即在**零假設**（通常記為$H_0$）的前提下，估算某事件發生的可能性，如果該事件是小機率事件，在一次試驗中本不應該發生，但現在卻發生了，此時我們就有足夠的理由懷疑零假設，轉而接受**備擇假設**（通常記為$H_A$）。

假設檢驗會存在兩種錯誤情況，一種稱為“拒真”，一種稱為“取偽”。如果原假設是對的，但你拒絕了原假設，這種錯誤就叫作“拒真”，這個錯誤的機率也叫作顯著性水平$\alpha$，或稱為容忍度；如果原假設是錯的，但你承認了原假設，這種錯誤就叫作“取偽”，這個錯誤的機率我們記為$\beta$。

### 總結

描述性統計通常用於研究表象，將現象用資料的方式描述出來（用整體的資料來描述整體的特徵）；推理性統計通常用於推測本質（透過樣本資料特徵去推理總體資料特徵），也就是你看到的表象的東西有多大機率符合你對隱藏在表象後的本質的猜測。
